{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b09e6a-de89-41df-a259-019b77724ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_absolute_percentage_error,\n",
    "    mean_squared_error,\n",
    "    root_mean_squared_error,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# local scripts\n",
    "from utils import (\n",
    "    add_noise,\n",
    "    get_train_test_splits,\n",
    "    load_pems,\n",
    "    split_time_series,\n",
    "    TSTrainDataset,\n",
    ")\n",
    "from encoder.tcn_encoder import CausalCNNEncoder\n",
    "from encoder.losses.info_nce import InfoNCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82bf933-df50-4e24-b03f-c21bd0a2ec90",
   "metadata": {},
   "source": [
    "### Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579c5b83-e3d9-4392-a2e5-da513acd6892",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        train_dataset,\n",
    "        lr=0.001,\n",
    "        batch_size=8,\n",
    "        device='cpu',\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.net = copy.deepcopy(encoder).to(device)\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer(self.net.parameters(), lr=lr)\n",
    "\n",
    "        self.train_dataset = train_dataset\n",
    "        self.train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "        )\n",
    "\n",
    "        self.n_epochs = 0\n",
    "        self.n_iters = 0\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        n_epochs=None,\n",
    "        n_iters=None,\n",
    "        mask_prob=0.5,\n",
    "        verbose=False,\n",
    "    ):\n",
    "        ''' `train_ds` shape: [B, C, T]\n",
    "        '''\n",
    "        ### Set the default number of training iterations to 600\n",
    "        if n_iters is None and n_epochs is None: n_iters = 600\n",
    "\n",
    "        ### Training loop\n",
    "        loss_log = []\n",
    "        while True:\n",
    "            if n_epochs is not None and self.n_epochs >= n_epochs: break\n",
    "\n",
    "            total_loss = 0\n",
    "            n_epoch_iters = 0\n",
    "\n",
    "            interrupted = False\n",
    "            ### Iterate through each mini-batch\n",
    "            for x in self.train_dataloader:\n",
    "                if n_iters is not None and self.n_iters >= n_iters:\n",
    "                    interrupted = True\n",
    "                    break\n",
    "\n",
    "                ### Reset gradient\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                ### Encode the input\n",
    "                x1 = self.net(x, mask_prob=0.75)\n",
    "                x2 = self.net(add_noise(x), mask_prob=1.0)\n",
    "\n",
    "                ### Calculate loss and backward pass\n",
    "                loss = self.loss_fn(\n",
    "                    x1.reshape(self.batch_size, -1),\n",
    "                    x2.reshape(self.batch_size, -1),\n",
    "                )\n",
    "                loss.backward()\n",
    "\n",
    "                ### Upgrade gradient\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                n_epoch_iters += 1\n",
    "                self.n_iters += 1\n",
    "\n",
    "            if interrupted: break\n",
    "\n",
    "            total_loss /= n_epoch_iters\n",
    "            loss_log.append(total_loss)\n",
    "\n",
    "            ### Print training loss\n",
    "            if verbose: print(f'Epoch {self.n_epochs}: loss = {total_loss}')\n",
    "\n",
    "            self.n_epochs += 1\n",
    "\n",
    "        return loss_log\n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        data,\n",
    "        batch_size=None,\n",
    "        mask=None,\n",
    "        pooling='max',\n",
    "    ):\n",
    "        ''' Input: [B, C, T]\n",
    "        '''\n",
    "        assert self.net is not None, 'Please train or load a model'\n",
    "        assert data.ndim == 3\n",
    "\n",
    "        if batch_size is None: batch_size = self.batch_size\n",
    "        n_samples, _, seq_len = data.shape\n",
    "\n",
    "        org_training = self.net.training\n",
    "        self.net.eval()\n",
    "\n",
    "        ### Dataset and DataLoader\n",
    "        dataset = TensorDataset(torch.from_numpy(data).float())\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "        ### encode data\n",
    "        with torch.no_grad():\n",
    "            output = []\n",
    "            for x in dataloader:\n",
    "                output.append(self._eval_with_pooling(x[0], mask, pooling))\n",
    "            output = torch.cat(output, dim=0).squeeze(2)\n",
    "        self.net.train(org_training)\n",
    "        return output.numpy()\n",
    "\n",
    "    def _eval_with_pooling(self, x, mask=None, pooling='max'):\n",
    "        ''' Input: [B, C, T]\n",
    "        '''\n",
    "        out = self.net(x.to(self.device, non_blocking=True), mask)\n",
    "        seq_len = out.size(2)\n",
    "        if pooling == 'max':\n",
    "            out = F.max_pool1d(out, kernel_size=seq_len)\n",
    "        elif pooling == 'avg':\n",
    "            out = F.avg_pool1d(out, kernel_size=seq_len)\n",
    "        elif pooling == 'max_avg' or pooling == 'avg_max':\n",
    "            out_max = F.max_pool1d(out, kernel_size=seq_len)\n",
    "            out_avg = F.avg_pool1d(out, kernel_size=seq_len)\n",
    "            out = torch.cat([out_max, out_avg], dim=1)\n",
    "        elif pooling == 'max+avg' or pooling == 'avg+max':\n",
    "            out_max = F.max_pool1d(out, kernel_size=seq_len)\n",
    "            out_avg = F.avg_pool1d(out, kernel_size=seq_len)\n",
    "            out = out_max + out_avg\n",
    "        else:\n",
    "            # default to max pooling\n",
    "            out = F.max_pool1d(out, kernel_size=seq_len)\n",
    "        return out.cpu()\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(torch.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c3f479-49fa-44d9-a68f-672fc01e36d2",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c0c28f-166a-4995-8622-127a763b75b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original shape: [timestep, location, feature]\n",
    "# reshape to:     [location, feature, timestep]\n",
    "# [B, C, T]\n",
    "pems = load_pems('data/pems04.npz')\n",
    "X_train, X_val, X_test = get_train_test_splits(pems)\n",
    "\n",
    "l = 12 * 24 * 7\n",
    "X_train = split_time_series(X_train, l)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f798d0a7-369d-4685-9e69-0a9c64371362",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_params = {\n",
    "    'hidden_dim': 64,\n",
    "    'output_dim': 256,\n",
    "    'depth': 6,\n",
    "    'kernel_size': 3,\n",
    "    'dropout': 0.2,\n",
    "    'mask_mode': 'b',\n",
    "}\n",
    "model_params = {\n",
    "    'loss_fn': InfoNCE(temperature=0.07),\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'train_dataset': TSTrainDataset(X_train),\n",
    "    'lr': 0.0001,\n",
    "    'batch_size': 16,\n",
    "    'device': 'cpu',\n",
    "}\n",
    "\n",
    "encoder = CausalCNNEncoder(input_dim=X_train.shape[1], **encoder_params)\n",
    "model = TSEncoder(encoder=encoder, **model_params)\n",
    "\n",
    "log = model.fit(\n",
    "    n_epochs=10,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1671568-c3f6-44f6-aa6b-476a5630a7df",
   "metadata": {},
   "source": [
    "### test stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a3aa4e-5cbf-4053-a223-87db3fb5743e",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 12 * 24\n",
    "t = 12\n",
    "\n",
    "X_train_repr = model.encode(X_test[:, :, :l-t], pooling='avg')\n",
    "X_test_repr = model.encode(X_test[:, :, :l], pooling='avg')\n",
    "\n",
    "y_train = X_test[:, 0, l-t:l]\n",
    "y_test = X_test[:, 0, l:l+t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91de0cd2-9e00-47c9-b0a7-de070eac0824",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = MLP(256, 64, t)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(mlp_model.parameters(), lr=0.001)\n",
    "\n",
    "n_epochs = 200\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = mlp_model(torch.tensor(X_train_repr).float())\n",
    "    loss = torch.sqrt(loss_fn(y_pred, torch.tensor(y_train).float()))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n",
    "\n",
    "mlp_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = mlp_model(torch.tensor(X_test_repr).float())\n",
    "\n",
    "print()\n",
    "rmse = root_mean_squared_error(y_test, y_pred.numpy())\n",
    "mae = mean_absolute_error(y_test, y_pred.numpy())\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred.numpy())\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'MAPE: {mape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b21884-3316-443a-ab04-405890302f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
