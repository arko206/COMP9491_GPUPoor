{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08b09e6a-de89-41df-a259-019b77724ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_absolute_percentage_error,\n",
    "    mean_squared_error,\n",
    "    root_mean_squared_error,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "# local scripts\n",
    "from utils import *\n",
    "from encoder.tcn_encoder import CausalCNNEncoder\n",
    "from encoder.losses.info_nce import InfoNCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dc4427-2484-4046-8eb1-7c9c9aa3310c",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "902b0b21-305d-4e2c-975b-89a4ef41d810",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSTrainDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.from_numpy(data).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "class TSTestDataset:\n",
    "    def __init__(self, encoder, data, hist_timesteps, forecast_steps, skip=12):\n",
    "        self.X, self.y = create_lag_features(\n",
    "            data, hist_timesteps, forecast_steps, skip,\n",
    "        )\n",
    "        n_samples = self.X.shape[0]\n",
    "        self.X = torch.tensor(np.array([model.encode(self.X[i], pooling='avg') for i in range(n_samples)])).float()\n",
    "        self.y = torch.tensor(self.y).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx].squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82bf933-df50-4e24-b03f-c21bd0a2ec90",
   "metadata": {},
   "source": [
    "### Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "579c5b83-e3d9-4392-a2e5-da513acd6892",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        train_dataset,\n",
    "        lr=0.001,\n",
    "        batch_size=8,\n",
    "        device='cpu',\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.net = copy.deepcopy(encoder).to(device)\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer(self.net.parameters(), lr=lr)\n",
    "\n",
    "        self.train_dataset = train_dataset\n",
    "        self.train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "        )\n",
    "\n",
    "        self.n_epochs = 0\n",
    "        self.n_iters = 0\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        n_epochs=None,\n",
    "        n_iters=None,\n",
    "        mask_prob=0.5,\n",
    "        verbose=False,\n",
    "    ):\n",
    "        ''' `train_ds` shape: [B, C, T]\n",
    "        '''\n",
    "        ### Set the default number of training iterations to 600\n",
    "        if n_iters is None and n_epochs is None: n_iters = 600\n",
    "\n",
    "        ### Training loop\n",
    "        loss_log = []\n",
    "        while True:\n",
    "            if n_epochs is not None and self.n_epochs >= n_epochs: break\n",
    "\n",
    "            total_loss = 0\n",
    "            n_epoch_iters = 0\n",
    "\n",
    "            interrupted = False\n",
    "            ### Iterate through each mini-batch\n",
    "            for x in self.train_dataloader:\n",
    "                if n_iters is not None and self.n_iters >= n_iters:\n",
    "                    interrupted = True\n",
    "                    break\n",
    "\n",
    "                ### Reset gradient\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                ### Get two cropped views\n",
    "                x1, x2, overlap_size = get_crops(x, overlap_size=0.7)\n",
    "\n",
    "                ### Encode the input\n",
    "                x1 = self.net(x1, mask_prob=0.5)[:, :, -overlap_size:]\n",
    "                x2 = self.net(x2, mask_prob=0.5)[:, :, :overlap_size]\n",
    "                # x2 = self.net(add_noise(x2, std=0.02), mask_prob=0.5)[:, :, :overlap_size]\n",
    "\n",
    "                ### Calculate loss and backward pass\n",
    "                loss = self.loss_fn(\n",
    "                    x1.reshape(self.batch_size, -1),\n",
    "                    x2.reshape(self.batch_size, -1),\n",
    "                )\n",
    "                loss.backward()\n",
    "\n",
    "                ### Upgrade gradient\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                n_epoch_iters += 1\n",
    "                self.n_iters += 1\n",
    "\n",
    "            if interrupted: break\n",
    "\n",
    "            total_loss /= n_epoch_iters\n",
    "            loss_log.append(total_loss)\n",
    "\n",
    "            ### Print training loss\n",
    "            if verbose: print(f'Epoch {self.n_epochs}: loss = {total_loss}')\n",
    "\n",
    "            self.n_epochs += 1\n",
    "\n",
    "        return loss_log\n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        data,\n",
    "        batch_size=None,\n",
    "        mask=None,\n",
    "        pooling='max',\n",
    "    ):\n",
    "        ''' Input: [B, C, T]\n",
    "        '''\n",
    "        assert self.net is not None, 'Please train or load a model'\n",
    "        assert data.ndim == 3\n",
    "\n",
    "        if batch_size is None: batch_size = self.batch_size\n",
    "        n_samples, _, seq_len = data.shape\n",
    "\n",
    "        org_training = self.net.training\n",
    "        self.net.eval()\n",
    "\n",
    "        ### Dataset and DataLoader\n",
    "        dataset = TensorDataset(torch.from_numpy(data).float())\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "        ### encode data\n",
    "        with torch.no_grad():\n",
    "            output = []\n",
    "            for x in dataloader:\n",
    "                output.append(self._eval_with_pooling(x[0], mask, pooling))\n",
    "            output = torch.cat(output, dim=0).squeeze(2)\n",
    "        self.net.train(org_training)\n",
    "        return output.numpy()\n",
    "\n",
    "    def _eval_with_pooling(self, x, mask=None, pooling='avg'):\n",
    "        ''' Input: [B, C, T]\n",
    "        '''\n",
    "        out = self.net(x.to(self.device, non_blocking=True), mask)\n",
    "        seq_len = out.size(2)\n",
    "        if pooling == 'avg':\n",
    "            out = F.avg_pool1d(out, kernel_size=seq_len)\n",
    "        elif pooling == 'max':\n",
    "            out = F.max_pool1d(out, kernel_size=seq_len)\n",
    "        return out.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c3f479-49fa-44d9-a68f-672fc01e36d2",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46c0c28f-166a-4995-8622-127a763b75b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original shape: [timestep, location, feature]\n",
    "# reshape to:     [location, feature, timestep]\n",
    "# [B, C, T]\n",
    "dataset = 'pems04'\n",
    "pems = load_pems(f'data/{dataset}.npz')\n",
    "X_train, X_val, X_test = get_train_test_splits(pems, train_size=0.6, test_size=0.2)\n",
    "\n",
    "l = 12 + 6\n",
    "X_train = split_time_series(X_train, l)\n",
    "selected_samples = np.random.permutation(np.arange(X_train.shape[0]))[:2000]\n",
    "X_train = X_train[selected_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f798d0a7-369d-4685-9e69-0a9c64371362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss = 2.323422065258026\n",
      "Epoch 1: loss = 2.0914916157722474\n",
      "Epoch 2: loss = 2.0729483942985536\n",
      "Epoch 3: loss = 2.0320279717445375\n",
      "Epoch 4: loss = 1.623516972064972\n",
      "Epoch 5: loss = 1.254003221988678\n",
      "Epoch 6: loss = 1.1857203299999237\n",
      "Epoch 7: loss = 1.0963209590911864\n",
      "Epoch 8: loss = 1.0469408822059632\n",
      "Epoch 9: loss = 0.9758206036090851\n"
     ]
    }
   ],
   "source": [
    "repr_dim = 128\n",
    "\n",
    "encoder_params = {\n",
    "    'hidden_dim': 32,\n",
    "    'output_dim': repr_dim,\n",
    "    'depth': 2,\n",
    "    'kernel_size': 3,\n",
    "    'dropout': 0.0,\n",
    "    'mask_mode': 'b',\n",
    "}\n",
    "model_params = {\n",
    "    'loss_fn': InfoNCE(temperature=0.1),\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'train_dataset': TSTrainDataset(X_train),\n",
    "    'lr': 0.0001,\n",
    "    'batch_size': 8,\n",
    "    'device': 'cpu',\n",
    "}\n",
    "\n",
    "encoder = CausalCNNEncoder(input_dim=X_train.shape[1], **encoder_params)\n",
    "model = TSEncoder(encoder=encoder, **model_params)\n",
    "\n",
    "log = model.fit(\n",
    "    n_epochs=10,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1671568-c3f6-44f6-aa6b-476a5630a7df",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1d46fb9-3335-4524-a4b9-c73079f0d3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145.69440021900118\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, X_test = get_train_test_splits(pems, train_size=0.6, test_size=0.2)\n",
    "\n",
    "hist_timesteps = 12\n",
    "forecast_steps = 12\n",
    "skip = forecast_steps\n",
    "\n",
    "start = timer()\n",
    "\n",
    "train_ds = TSTestDataset(model, X_train, hist_timesteps, forecast_steps, skip)\n",
    "test_ds = TSTestDataset(model, X_test, hist_timesteps, forecast_steps, skip)\n",
    "\n",
    "end = timer()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a00544c-c4bd-4180-b9f9-623f7d6284a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.264885019467116\n",
      "25.463049050281416\n"
     ]
    }
   ],
   "source": [
    "rmse = []\n",
    "mae = []\n",
    "\n",
    "for node in range(X_test.shape[0]):\n",
    "\n",
    "    X_train_single = train_ds[:][0][:, node]\n",
    "    y_train_single = train_ds[:][1][:, node]\n",
    "    X_test_single = test_ds[:][0][:, node]\n",
    "    y_test_single = test_ds[:][1][:, node]\n",
    "\n",
    "    lr = Ridge(alpha=1.0)\n",
    "\n",
    "    lr.fit(X_train_single, y_train_single)\n",
    "    y_pred = lr.predict(X_test_single)\n",
    "\n",
    "    rmse.append(root_mean_squared_error(y_pred, y_test_single))\n",
    "    mae.append(mean_absolute_error(y_pred, y_test_single))\n",
    "\n",
    "print(np.array(rmse).mean())\n",
    "print(np.array(mae).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "188c8398-a671-4272-a2d0-0b37ea33ef29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.21645359800008\n",
      "27.820573119643193\n"
     ]
    }
   ],
   "source": [
    "X, y = create_lag_features(X_train, hist_timesteps, forecast_steps, skip)\n",
    "X_t, y_t = create_lag_features(X_test, hist_timesteps, forecast_steps, skip)\n",
    "X = X.squeeze()\n",
    "X_t = X_t.squeeze()\n",
    "\n",
    "rmse = []\n",
    "mae = []\n",
    "\n",
    "for node in range(X_test.shape[0]):\n",
    "\n",
    "    X_train_single = X[:, node]\n",
    "    y_train_single = y[:, node]\n",
    "    X_test_single = X_t[:, node]\n",
    "    y_test_single = y_t[:, node]\n",
    "\n",
    "    lr = Ridge(alpha=1.0)\n",
    "    lr.fit(X_train_single, y_train_single)\n",
    "    y_pred = lr.predict(X_test_single)\n",
    "\n",
    "    rmse.append(root_mean_squared_error(y_pred, y_test_single))\n",
    "    mae.append(mean_absolute_error(y_pred, y_test_single))\n",
    "\n",
    "print(np.array(rmse).mean())\n",
    "print(np.array(mae).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f21a54-09c8-4bd1-bad5-fd81d6d5d49a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
