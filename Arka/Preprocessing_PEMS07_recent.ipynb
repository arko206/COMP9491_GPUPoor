{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beb73712-3618-4019-8c5b-f14b8513d70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0d0bbf-43ab-4215-9c91-300b10798989",
   "metadata": {},
   "source": [
    "##### Loading the Original Data from PEMS07 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4473ff5d-e23a-476b-be81-4f4c58e3ceb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28224, 883, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_signal_matrix_filename = r\"C:\\Users\\User\\Traffic_Flow_Forecasting\\ASTGCN_Baseline_Model\\Dataset_PEMS07\\PEMS07.npz\\PEMS07.npz\"\n",
    "data = np.load(graph_signal_matrix_filename)\n",
    "data['data'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd75199-f51d-4dcb-b865-3c3708f21992",
   "metadata": {},
   "source": [
    "#### The above dataset PEMS07 has 883 traffic sensors recording data with a duration of 3 months "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b98dd27b-45b0-47a2-a960-0924e361faa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The required number of days for which the data is collected is  98.0\n"
     ]
    }
   ],
   "source": [
    "number_of_days = 28224 / (12 * 24)\n",
    "print(\"The required number of days for which the data is collected is \", number_of_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca7f25d-e056-4135-966f-83ace5396ce3",
   "metadata": {},
   "source": [
    "##### the 1 hour data(INPUT) is of shape (12, 883, 1) and the next hour(TARGET) is the same (12, 883, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d624d4-3ef1-46a3-9371-bc5332942620",
   "metadata": {},
   "source": [
    "#### The function search_data returns a certain number of time period segment data of duration = 1hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4311b68-1fea-457c-888b-687c38c45b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_data(sequence_length, num_of_depend, label_start_idx,num_for_predict, units, points_per_hour):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    sequence_length: int, length of all history data\n",
    "    num_of_depend: int, number of dependent time periods of length 1 hour\n",
    "    label_start_idx: int, the first index of predicting target\n",
    "    num_for_predict: int, the number of points will be predicted for each sample (in case of a hour, for each input sample 12 points to\n",
    "                                                                                   be predicted)\n",
    "    units: int, week: 7 * 24, day: 24, recent(hour): 1\n",
    "    \n",
    "    points_per_hour: int, number of points per hour, depends on data, here 12 points are considered because 1 hour is considered\n",
    "    Returns\n",
    "    ----------\n",
    "    list[(start_idx, end_idx)]\n",
    "    '''\n",
    "    ###Checks if there are non-zero number of points in target sample\n",
    "    if points_per_hour < 0:\n",
    "        raise ValueError(\"points_per_hour should be greater than 0!\")\n",
    "\n",
    "    ###Checking if the selected sequence is bypassing the length of sequence\n",
    "    if label_start_idx + num_for_predict > sequence_length:\n",
    "        return None\n",
    "\n",
    "    x_idx = []\n",
    "    for i in range(1, num_of_depend + 1):\n",
    "        start_idx = label_start_idx - points_per_hour * units * i\n",
    "        end_idx = start_idx + num_for_predict\n",
    "        if start_idx >= 0:\n",
    "            x_idx.append((start_idx, end_idx))\n",
    "        else:\n",
    "            return None\n",
    "    ###Checking if we have same number of time periods or not\n",
    "    if len(x_idx) != num_of_depend:\n",
    "        return None\n",
    "    ### return in from of recent time period segmnent order\n",
    "    return x_idx[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eae0be-9b99-442d-bba1-627c911591af",
   "metadata": {},
   "source": [
    "### The Function 'get_sample_indices' returns the sample data concatenatedwith input (sequence length data), output (target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de7a41d7-e307-4bc7-bb8d-bd4a47aca752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_indices(data_sequence, num_of_weeks, num_of_days, num_of_hours, label_start_idx, num_for_predict, points_per_hour=12):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_sequence: np.ndarray shape is (sequence_length, num_of_vertices, num_of_features)\n",
    "    num_of_weeks, num_of_days, num_of_hours: int\n",
    "    label_start_idx: int, the first index of predicting target\n",
    "    num_for_predict: int,the number of points will be predicted for each sample\n",
    "    points_per_hour: int, default 12, number of points per hour\n",
    "    Returns\n",
    "    ----------\n",
    "    week_sample: np.ndarray shape is (num_of_weeks * points_per_hour, num_of_vertices, num_of_features)\n",
    "    day_sample: np.ndarray shape is (num_of_days * points_per_hour,  num_of_vertices, num_of_features)\n",
    "    hour_sample: np.ndarray   shape is (num_of_hours * points_per_hour, num_of_vertices, num_of_features)\n",
    "    target: np.ndarray shape is (num_for_predict, num_of_vertices, num_of_features)\n",
    "    '''\n",
    "    week_sample, day_sample, hour_sample = None, None, None\n",
    "#------------------------------------Ignore\n",
    "    ###Checks if the length of the predicting segment is outside the sequence lengtrg\n",
    "    if label_start_idx + num_for_predict > data_sequence.shape[0]: \n",
    "        return week_sample, day_sample, hour_sample, None\n",
    "\n",
    "    if num_of_weeks > 0:\n",
    "        week_indices = search_data(data_sequence.shape[0], num_of_weeks, label_start_idx, num_for_predict,7 * 24, points_per_hour)\n",
    "        if not week_indices:\n",
    "            return None, None, None, None\n",
    "\n",
    "        week_sample = np.concatenate([data_sequence[i: j] for i, j in week_indices], axis=0)\n",
    "\n",
    "    if num_of_days > 0:\n",
    "        day_indices = search_data(data_sequence.shape[0], num_of_days,  label_start_idx, num_for_predict, 24, points_per_hour)\n",
    "        if not day_indices:\n",
    "            return None, None, None, None\n",
    "\n",
    "        day_sample = np.concatenate([data_sequence[i: j] for i, j in day_indices], axis=0)\n",
    "#----------------------------------Continue\n",
    "\n",
    "    ###Checks if the number of hours is non-zero\n",
    "    if num_of_hours > 0:\n",
    "        hour_indices = search_data(data_sequence.shape[0], num_of_hours, label_start_idx, num_for_predict, 1, points_per_hour)\n",
    "        if not hour_indices:\n",
    "            return None, None, None, None\n",
    "\n",
    "        ###Getting the concatenated data along the sequence length axis\n",
    "        hour_sample = np.concatenate([data_sequence[i: j] for i, j in hour_indices], axis=0)\n",
    "    \n",
    "    if num_of_hours > 10:\n",
    "        return 1;\n",
    "    ###Getting the predicting data\n",
    "    target = data_sequence[label_start_idx: label_start_idx + num_for_predict]\n",
    "\n",
    "    return week_sample, day_sample, hour_sample, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cc49a0d-c004-4ff9-9821-690e4b68853d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_generate_dataset(graph_signal_matrix_filename, num_of_weeks, num_of_days, num_of_hours, num_for_predict, points_per_hour=12):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    graph_signal_matrix_filename: str, path of graph signal matrix file\n",
    "    num_of_weeks, num_of_days, num_of_hours: int\n",
    "    num_for_predict: int\n",
    "    points_per_hour: int, default 12, depends on data\n",
    "    Returns\n",
    "    ----------\n",
    "    feature: np.ndarray, shape is (num_of_samples, num_of_depend * points_per_hour, num_of_vertices, num_of_features)\n",
    "    target: np.ndarray, shape is (num_of_samples, num_of_vertices, num_for_predict)\n",
    "    '''\n",
    "    #--------------------------------- Read original data \n",
    "    data_seq = np.load(graph_signal_matrix_filename)['data']  # (sequence_length, num_of_vertices, num_of_features) (16992, 307, 3)\n",
    "    \n",
    "    #---------------------------------\n",
    "    all_samples = []\n",
    "    for idx in range(data_seq.shape[0]):\n",
    "        sample = get_sample_indices(data_seq, num_of_weeks, num_of_days, num_of_hours, idx, num_for_predict, points_per_hour)\n",
    "        if ((sample[0] is None) and (sample[1] is None) and (sample[2] is None)):\n",
    "            continue\n",
    "\n",
    "        week_sample, day_sample, hour_sample, target = sample #  week_sample, day_sample are None because we are predicting per hour\n",
    "        #print(target.shape) # hour_sample and target (12, 307, 3)\n",
    "        sample = []  # [(week_sample),(day_sample),(hour_sample),target,time_sample]\n",
    "#-------------------------------- Ignore\n",
    "        if num_of_weeks > 0:\n",
    "            week_sample = np.expand_dims(week_sample, axis=0).transpose((0, 2, 3, 1))  # (1,N,F,T)\n",
    "            sample.append(week_sample)\n",
    "\n",
    "        if num_of_days > 0:\n",
    "            day_sample = np.expand_dims(day_sample, axis=0).transpose((0, 2, 3, 1))  # (1,N,F,T)\n",
    "            sample.append(day_sample)\n",
    "#----------------------------------Continue\n",
    "        if num_of_hours > 0:\n",
    "            hour_sample = np.expand_dims(hour_sample, axis=0).transpose((0, 2, 3, 1))  # (1,N,F,T)\n",
    "            sample.append(hour_sample)\n",
    "        ####Considering only the First Feature\n",
    "        target = np.expand_dims(target, axis=0).transpose((0, 2, 3, 1))[:, :, 0, :]  # (1,N,T)\n",
    "        sample.append(target)\n",
    "        time_sample = np.expand_dims(np.array([idx]), axis=0)  # (1,1)\n",
    "        sample.append(time_sample)\n",
    "        all_samples.append(sample)#sampe：[(week_sample),(day_sample),(hour_sample),target,time_sample] = [(1,N,F,Tw),(1,N,F,Td),(1,N,F,Th),(1,N,Tpre),(1,1)]\n",
    "\n",
    "    split_line1 = int(len(all_samples) * 0.6)\n",
    "    split_line2 = int(len(all_samples) * 0.8)\n",
    "\n",
    "    training_set = [np.concatenate(i, axis=0)  for i in zip(*all_samples[:split_line1])] #[(B,N,F,Tw),(B,N,F,Td),(B,N,F,Th),(B,N,Tpre),(B,1)]\n",
    "    validation_set = [np.concatenate(i, axis=0) for i in zip(*all_samples[split_line1: split_line2])]\n",
    "    testing_set = [np.concatenate(i, axis=0) for i in zip(*all_samples[split_line2:])]\n",
    "\n",
    "    return training_set, validation_set, testing_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "818295a6-e03a-4c76-95f4-dcf84334942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(graph_signal_matrix_filename)\n",
    "data['data'].shape\n",
    "\n",
    "num_of_vertices = 883\n",
    "points_per_hour = 12\n",
    "num_for_predict = 12\n",
    "num_of_weeks = 0\n",
    "num_of_days = 0\n",
    "num_of_hours = 1\n",
    "\n",
    "####Generating the training, test and validation dataset\n",
    "training_set, validation_set, testing_set = read_and_generate_dataset(graph_signal_matrix_filename, 0, 0, num_of_hours, \n",
    "                                                                      num_for_predict, points_per_hour=points_per_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f9709f6-2bc5-4c30-a707-af336b0c7f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de61354f-0d72-426a-bf87-095c21518335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16920, 883, 1, 12)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce393847-c5e3-4ef1-94f2-de59f32ee8e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16920, 883, 12)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f992fce-ae92-47e7-8e23-a23b647e3949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16920, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80268500-ac16-4d22-99d7-bd64c65c00e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[246., 249., 243., ..., 169., 178., 182.]],\n",
       "\n",
       "       [[193., 156., 158., ..., 148., 112., 154.]],\n",
       "\n",
       "       [[232., 204., 209., ..., 154., 152., 167.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[295., 263., 276., ..., 182., 181., 182.]],\n",
       "\n",
       "       [[ 85., 109.,  87., ...,  79.,  83.,  88.]],\n",
       "\n",
       "       [[259., 243., 244., ..., 182., 190., 164.]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a89f4a19-d160-4ab5-b84e-37b2e35a62f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(883, 1, 12)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bcc4c1e8-f721-4949-bfee-514abdf86be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[175., 210., 130., ..., 144., 143., 143.],\n",
       "       [141., 126., 136., ..., 113., 110.,  99.],\n",
       "       [161., 149., 126., ..., 135., 115., 115.],\n",
       "       ...,\n",
       "       [145., 144., 151., ..., 125., 129., 121.],\n",
       "       [ 76.,  82.,  72., ...,  73.,  79.,  63.],\n",
       "       [176., 171., 141., ..., 111., 126., 121.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e09382d4-e7cb-4b0f-bcfc-4e25b567fffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(883, 12)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6100aed1-158d-4fc5-8d9b-1c810039dd88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[2][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a23ce01-8512-466a-be2e-4b6cbf636d0c",
   "metadata": {},
   "source": [
    "### Data Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2396d5b-ee5d-4664-97ca-1eeeff2588a2",
   "metadata": {},
   "source": [
    "###### the data are transformed by zero-mean normalization x′ = x −mean(x) to let the average be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d95ac3b8-85e6-4d44-bb4e-1af72bba144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(train, val, test):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    train, val, test: np.ndarray (B,N,F,T)\n",
    "    Returns\n",
    "    ----------\n",
    "    stats: dict, two keys: mean and std\n",
    "    train_norm, val_norm, test_norm: np.ndarray,\n",
    "                                     shape is the same as original\n",
    "    '''\n",
    "\n",
    "    assert train.shape[1:] == val.shape[1:] and val.shape[1:] == test.shape[1:]  # ensure the num of nodes is the same\n",
    "    mean = train.mean(axis=(0,1,3), keepdims=True)\n",
    "    std = train.std(axis=(0,1,3), keepdims=True)\n",
    "    print('mean.shape:',mean.shape)\n",
    "    print('std.shape:',std.shape)\n",
    "\n",
    "    def normalize(x):\n",
    "        return (x - mean) / std\n",
    "\n",
    "    train_norm = normalize(train)\n",
    "    val_norm = normalize(val)\n",
    "    test_norm = normalize(test)\n",
    "\n",
    "    return {'_mean': mean, '_std': std}, train_norm, val_norm, test_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "869337df-a978-4a1d-aae5-a1f4ac0e2cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean.shape: (1, 1, 1, 1)\n",
      "std.shape: (1, 1, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "train_x = np.concatenate(training_set[:-2], axis=-1)  # (B,N,F,T')\n",
    "val_x = np.concatenate(validation_set[:-2], axis=-1)\n",
    "test_x = np.concatenate(testing_set[:-2], axis=-1)\n",
    "\n",
    "train_target = training_set[-2]  # (B,N,T)\n",
    "val_target = validation_set[-2]\n",
    "test_target = testing_set[-2]\n",
    "\n",
    "train_timestamp = training_set[-1]  # (B,1)\n",
    "val_timestamp = validation_set[-1]\n",
    "test_timestamp = testing_set[-1]\n",
    "\n",
    "(stats, train_x_norm, val_x_norm, test_x_norm) = normalization(train_x, val_x, test_x)\n",
    "\n",
    "all_data = {'train': { 'x': train_x_norm, 'target': train_target,'timestamp': train_timestamp},\n",
    "            'val': {'x': val_x_norm, 'target': val_target, 'timestamp': val_timestamp},\n",
    "            'test': {'x': test_x_norm, 'target': test_target, 'timestamp': test_timestamp},\n",
    "            'stats': {'_mean': stats['_mean'], '_std': stats['_std']} }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ef908a35-c2db-4287-bf5f-99a6dc0fad8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train x: (16920, 883, 1, 12)\n",
      "train target: (16920, 883, 12)\n",
      "train timestamp: (16920, 1)\n",
      "\n",
      "val x: (5640, 883, 1, 12)\n",
      "val target: (5640, 883, 12)\n",
      "val timestamp: (5640, 1)\n",
      "\n",
      "test x: (5641, 883, 1, 12)\n",
      "test target: (5641, 883, 12)\n",
      "test timestamp: (5641, 1)\n",
      "\n",
      "train data _mean : (1, 1, 1, 1) [[[[309.53543959]]]]\n",
      "train data _std : (1, 1, 1, 1) [[[[189.50781284]]]]\n"
     ]
    }
   ],
   "source": [
    "print('train x:', all_data['train']['x'].shape)\n",
    "print('train target:', all_data['train']['target'].shape)\n",
    "print('train timestamp:', all_data['train']['timestamp'].shape)\n",
    "print()\n",
    "print('val x:', all_data['val']['x'].shape)\n",
    "print('val target:', all_data['val']['target'].shape)\n",
    "print('val timestamp:', all_data['val']['timestamp'].shape)\n",
    "print()\n",
    "print('test x:', all_data['test']['x'].shape)\n",
    "print('test target:', all_data['test']['target'].shape)\n",
    "print('test timestamp:', all_data['test']['timestamp'].shape)\n",
    "print()\n",
    "print('train data _mean :', all_data['stats']['_mean'].shape, all_data['stats']['_mean'])\n",
    "print('train data _std :', all_data['stats']['_std'].shape, all_data['stats']['_std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e6f71140-6b85-49c7-9b66-c35384d09dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save file: .\\PEMS07_r1_d0_w0_astcgn\n"
     ]
    }
   ],
   "source": [
    "file = os.path.basename(graph_signal_matrix_filename).split('.')[0]\n",
    "dirpath = '.'\n",
    "filename = os.path.join(dirpath, file + '_r' + str(num_of_hours) + '_d' + str(num_of_days) + '_w' + str(num_of_weeks)) + '_astcgn'\n",
    "print('save file:', filename)\n",
    "np.savez_compressed(filename,\n",
    "                train_x=all_data['train']['x'],train_target=all_data['train']['target'],train_timestamp=all_data['train']['timestamp'],\n",
    "                val_x=all_data['val']['x'], val_target=all_data['val']['target'],val_timestamp=all_data['val']['timestamp'],\n",
    "                test_x=all_data['test']['x'], test_target=all_data['test']['target'], test_timestamp=all_data['test']['timestamp'],\n",
    "                mean=all_data['stats']['_mean'], std=all_data['stats']['_std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fc3e60-46b9-4ae3-88a9-8289c97c3de8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
