{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beb73712-3618-4019-8c5b-f14b8513d70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0d0bbf-43ab-4215-9c91-300b10798989",
   "metadata": {},
   "source": [
    "##### Loading the Original Data from PEMS07 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4473ff5d-e23a-476b-be81-4f4c58e3ceb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16992, 307, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_signal_matrix_filename = r\"C:\\Users\\User\\Traffic_Flow_Forecasting\\ASTGCN_Baseline_Model\\PEMS04\\PEMS04.npz\"\n",
    "data = np.load(graph_signal_matrix_filename)\n",
    "data['data'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca7f25d-e056-4135-966f-83ace5396ce3",
   "metadata": {},
   "source": [
    "##### the 30 minutes data(INPUT) is of shape (12, 883, 1) and the next hour(TARGET) is the same (6, 883, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d624d4-3ef1-46a3-9371-bc5332942620",
   "metadata": {},
   "source": [
    "#### The function search_data returns a certain number of time period segment data of duration = 1hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4311b68-1fea-457c-888b-687c38c45b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_data(sequence_length, num_of_depend, label_start_idx,num_for_predict, units, points_per_half_hour):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    sequence_length: int, length of all history data\n",
    "    num_of_depend: int, number of dependent time periods of length 1 hour\n",
    "    label_start_idx: int, the first index of predicting target\n",
    "    num_for_predict: int, the number of points will be predicted for each sample (in case of a hour, for each input sample 12 points to\n",
    "                                                                                   be predicted)\n",
    "    units: int, week: 7 * 24, day: 24, recent(hour): 1\n",
    "    \n",
    "    points_per_hour: int, number of points per hour, depends on data, here 12 points are considered because 1 hour is considered\n",
    "    Returns\n",
    "    ----------\n",
    "    list[(start_idx, end_idx)]\n",
    "    '''\n",
    "    ###Checks if there are non-zero number of points in target sample\n",
    "    if points_per_half_hour < 0:\n",
    "        raise ValueError(\"points_per_hour should be greater than 0!\")\n",
    "\n",
    "    ###Checking if the selected sequence is bypassing the length of sequence\n",
    "    if label_start_idx + num_for_predict > sequence_length:\n",
    "        return None\n",
    "\n",
    "    x_idx = []\n",
    "    for i in range(1, num_of_depend + 1):\n",
    "        start_idx = label_start_idx - points_per_half_hour * units * i\n",
    "        end_idx = start_idx + num_for_predict\n",
    "        if start_idx >= 0:\n",
    "            x_idx.append((start_idx, end_idx))\n",
    "        else:\n",
    "            return None\n",
    "    ###Checking if we have same number of time periods or not\n",
    "    if len(x_idx) != num_of_depend:\n",
    "        return None\n",
    "    ### return in from of recent time period segmnent order\n",
    "    return x_idx[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eae0be-9b99-442d-bba1-627c911591af",
   "metadata": {},
   "source": [
    "### The Function 'get_sample_indices' returns the sample data concatenatedwith input (sequence length data), output (target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de7a41d7-e307-4bc7-bb8d-bd4a47aca752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_indices(data_sequence, num_of_weeks, num_of_days, num_of_half_hours, label_start_idx, num_for_predict, points_per_half_hour=6):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_sequence: np.ndarray shape is (sequence_length, num_of_vertices, num_of_features)\n",
    "    num_of_weeks, num_of_days, num_of_hours: int\n",
    "    label_start_idx: int, the first index of predicting target\n",
    "    num_for_predict: int, the number of points will be predicted for each sample\n",
    "    points_per_half_hour: int, default 6, number of points in 30 mins\n",
    "    Returns\n",
    "    ----------\n",
    "    week_sample: np.ndarray shape is (num_of_weeks * points_per_hour, num_of_vertices, num_of_features)\n",
    "    day_sample: np.ndarray shape is (num_of_days * points_per_hour,  num_of_vertices, num_of_features)\n",
    "    half_hour_sample: np.ndarray shape is (num_of_half_hour * points_per_half_hour, num_of_vertices, num_of_features)\n",
    "    target: np.ndarray shape is (num_for_predict, num_of_vertices, num_of_features)\n",
    "    '''\n",
    "    week_sample, day_sample, half_hour_sample = None, None, None\n",
    "\n",
    "    if label_start_idx + num_for_predict > data_sequence.shape[0]:\n",
    "        return None, None, None, None\n",
    "\n",
    "    if num_of_half_hours > 0:\n",
    "        half_hour_indices = search_data(data_sequence.shape[0], num_of_half_hours, label_start_idx, num_for_predict, 1, points_per_half_hour)\n",
    "        if not half_hour_indices:\n",
    "            return None, None, None, None\n",
    "\n",
    "        half_hour_sample = np.concatenate([data_sequence[i: j] for i, j in half_hour_indices], axis=0)\n",
    "    \n",
    "\n",
    "    if num_of_days > 0:\n",
    "        day_indices = search_data(data_sequence.shape[0], num_of_days, label_start_idx, num_for_predict, 24, points_per_hour)\n",
    "        if not day_indices:\n",
    "            return None, None, None, None\n",
    "        day_sample = np.concatenate([data_sequence[i: j] for i, j in day_indices], axis=0)\n",
    "\n",
    "    if num_of_weeks > 0:\n",
    "        week_indices = search_data(data_sequence.shape[0], num_of_weeks, label_start_idx, num_for_predict, 7 * 24, points_per_hour)\n",
    "        if not week_indices:\n",
    "            return None, None, None, None\n",
    "        week_sample = np.concatenate([data_sequence[i: j] for i, j in week_indices], axis=0)\n",
    "\n",
    "    target = data_sequence[label_start_idx: label_start_idx + num_for_predict]\n",
    "\n",
    "    return week_sample, day_sample, half_hour_sample, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cc49a0d-c004-4ff9-9821-690e4b68853d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_generate_dataset(graph_signal_matrix_filename, num_of_weeks, num_of_days, num_of_half_hours, num_for_predict, points_per_half_hour=6):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    graph_signal_matrix_filename: str, path of graph signal matrix file\n",
    "    num_of_weeks, num_of_days, num_of_hours: int\n",
    "    num_for_predict: int\n",
    "    points_per_half_hour: int, default 6, depends on data\n",
    "    Returns\n",
    "    ----------\n",
    "    feature: np.ndarray, shape is (num_of_samples, num_of_depend * points_per_hour, num_of_vertices, num_of_features)\n",
    "    target: np.ndarray, shape is (num_of_samples, num_of_vertices, num_for_predict)\n",
    "    '''\n",
    "    #--------------------------------- Read original data \n",
    "    data_seq = np.load(graph_signal_matrix_filename)['data']  # (sequence_length, num_of_vertices, num_of_features) (16992, 307, 3)\n",
    "    \n",
    "    #---------------------------------\n",
    "    all_samples = []\n",
    "    for idx in range(data_seq.shape[0]):\n",
    "        sample = get_sample_indices(data_seq, num_of_weeks, num_of_days, num_of_half_hours, idx, num_for_predict, points_per_half_hour)\n",
    "        if ((sample[0] is None) and (sample[1] is None) and (sample[2] is None)):\n",
    "            continue\n",
    "\n",
    "        week_sample, day_sample, half_hour_sample, target = sample #  week_sample, day_sample are None because we are predicting per hour\n",
    "        #print(target.shape) # hour_sample and target (12, 307, 3)\n",
    "        sample = []  # [(week_sample),(day_sample),(hour_sample),target,time_sample]\n",
    "#-------------------------------- Ignore\n",
    "        if num_of_weeks > 0:\n",
    "            week_sample = np.expand_dims(week_sample, axis=0).transpose((0, 2, 3, 1))  # (1,N,F,T)\n",
    "            sample.append(week_sample)\n",
    "\n",
    "        if num_of_days > 0:\n",
    "            day_sample = np.expand_dims(day_sample, axis=0).transpose((0, 2, 3, 1))  # (1,N,F,T)\n",
    "            sample.append(day_sample)\n",
    "\n",
    "        if num_of_half_hours > 0:\n",
    "            half_hour_sample = np.expand_dims(half_hour_sample, axis=0).transpose((0, 2, 3, 1))  # (1,N,F,T)\n",
    "            sample.append(half_hour_sample)\n",
    "        \n",
    "        target = np.expand_dims(target, axis=0).transpose((0, 2, 3, 1))[:, :, 0, :]  # (1,N,T)\n",
    "        sample.append(target)\n",
    "        time_sample = np.expand_dims(np.array([idx]), axis=0)  # (1,1)\n",
    "        sample.append(time_sample)\n",
    "        all_samples.append(sample)#sampeï¼š[(week_sample),(day_sample),(hour_sample),target,time_sample] = [(1,N,F,Tw),(1,N,F,Td),(1,N,F,Th),(1,N,Tpre),(1,1)]\n",
    "\n",
    "    split_line1 = int(len(all_samples) * 0.6)\n",
    "    split_line2 = int(len(all_samples) * 0.8)\n",
    "\n",
    "    training_set = [np.concatenate(i, axis=0)  for i in zip(*all_samples[:split_line1])] #[(B,N,F,Tw),(B,N,F,Td),(B,N,F,Th),(B,N,Tpre),(B,1)]\n",
    "    validation_set = [np.concatenate(i, axis=0) for i in zip(*all_samples[split_line1: split_line2])]\n",
    "    testing_set = [np.concatenate(i, axis=0) for i in zip(*all_samples[split_line2:])]\n",
    "\n",
    "    return training_set, validation_set, testing_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b97f439-f58f-4b88-adce-915d4f5f5eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_vertices = 307\n",
    "points_per_half_hour = 6\n",
    "num_for_predict = 6\n",
    "num_of_weeks = 0\n",
    "num_of_days = 0\n",
    "\n",
    "###Such that the input sequence consists of\n",
    "### data of consecutive 30 mins\n",
    "num_of_half_hours = 2\n",
    "\n",
    "# Generating the training, test and validation dataset\n",
    "training_set, validation_set, testing_set = read_and_generate_dataset(\n",
    "    graph_signal_matrix_filename, \n",
    "    num_of_weeks, \n",
    "    num_of_days, \n",
    "    num_of_half_hours, \n",
    "    num_for_predict, \n",
    "    points_per_half_hour=points_per_half_hour\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f9709f6-2bc5-4c30-a707-af336b0c7f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_set)##training set contains input, target and timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de61354f-0d72-426a-bf87-095c21518335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10185, 307, 3, 12)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce393847-c5e3-4ef1-94f2-de59f32ee8e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10185, 307, 6)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f992fce-ae92-47e7-8e23-a23b647e3949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10185, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80268500-ac16-4d22-99d7-bd64c65c00e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[6.20e+01, 6.10e+01, 7.10e+01, ..., 1.64e+02, 1.43e+02,\n",
       "         1.42e+02],\n",
       "        [7.70e-03, 7.40e-03, 9.30e-03, ..., 2.15e-02, 1.91e-02,\n",
       "         1.90e-02],\n",
       "        [6.79e+01, 6.73e+01, 6.84e+01, ..., 6.57e+01, 6.59e+01,\n",
       "         6.67e+01]],\n",
       "\n",
       "       [[5.60e+01, 4.00e+01, 2.80e+01, ..., 1.51e+02, 1.07e+02,\n",
       "         1.38e+02],\n",
       "        [1.12e-02, 8.00e-03, 6.80e-03, ..., 3.40e-02, 2.69e-02,\n",
       "         3.61e-02],\n",
       "        [6.84e+01, 6.86e+01, 6.74e+01, ..., 6.40e+01, 6.33e+01,\n",
       "         6.04e+01]],\n",
       "\n",
       "       [[9.00e+01, 9.40e+01, 7.90e+01, ..., 1.58e+02, 1.39e+02,\n",
       "         9.30e+01],\n",
       "        [1.43e-02, 1.56e-02, 1.22e-02, ..., 2.69e-02, 2.34e-02,\n",
       "         1.61e-02],\n",
       "        [6.82e+01, 6.78e+01, 6.87e+01, ..., 6.71e+01, 6.67e+01,\n",
       "         6.68e+01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[5.60e+01, 5.10e+01, 6.70e+01, ..., 1.82e+02, 2.25e+02,\n",
       "         1.72e+02],\n",
       "        [9.80e-03, 9.40e-03, 1.17e-02, ..., 3.23e-02, 3.87e-02,\n",
       "         2.96e-02],\n",
       "        [6.74e+01, 6.60e+01, 6.67e+01, ..., 6.53e+01, 6.52e+01,\n",
       "         6.55e+01]],\n",
       "\n",
       "       [[4.80e+01, 3.00e+01, 3.10e+01, ..., 1.12e+02, 1.14e+02,\n",
       "         9.40e+01],\n",
       "        [7.80e-03, 5.00e-03, 5.30e-03, ..., 1.89e-02, 1.94e-02,\n",
       "         1.53e-02],\n",
       "        [6.95e+01, 6.91e+01, 6.88e+01, ..., 6.86e+01, 6.82e+01,\n",
       "         6.88e+01]],\n",
       "\n",
       "       [[3.80e+01, 3.50e+01, 4.50e+01, ..., 1.18e+02, 1.26e+02,\n",
       "         1.12e+02],\n",
       "        [9.40e-03, 8.50e-03, 1.11e-02, ..., 3.08e-02, 3.34e-02,\n",
       "         2.91e-02],\n",
       "        [6.82e+01, 6.88e+01, 6.95e+01, ..., 6.73e+01, 6.64e+01,\n",
       "         6.63e+01]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a89f4a19-d160-4ab5-b84e-37b2e35a62f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307, 3, 12)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bcc4c1e8-f721-4949-bfee-514abdf86be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[145., 162., 126., 135., 138., 114.],\n",
       "       [141., 102., 127., 129., 101., 140.],\n",
       "       [132.,  96., 109.,  66.,  60., 147.],\n",
       "       ...,\n",
       "       [190., 173., 154., 173., 148., 183.],\n",
       "       [101.,  96.,  88.,  62.,  90.,  83.],\n",
       "       [106.,  83., 100., 102., 109.,  84.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e09382d4-e7cb-4b0f-bcfc-4e25b567fffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307, 6)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6100aed1-158d-4fc5-8d9b-1c810039dd88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[2][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a23ce01-8512-466a-be2e-4b6cbf636d0c",
   "metadata": {},
   "source": [
    "### Data Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2396d5b-ee5d-4664-97ca-1eeeff2588a2",
   "metadata": {},
   "source": [
    "###### the data are transformed by zero-mean normalization xâ€² = x âˆ’mean(x) to let the average be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d95ac3b8-85e6-4d44-bb4e-1af72bba144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(train, val, test):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    train, val, test: np.ndarray (B,N,F,T)\n",
    "    Returns\n",
    "    ----------\n",
    "    stats: dict, two keys: mean and std\n",
    "    train_norm, val_norm, test_norm: np.ndarray,\n",
    "                                     shape is the same as original\n",
    "    '''\n",
    "\n",
    "    assert train.shape[1:] == val.shape[1:] and val.shape[1:] == test.shape[1:]  # ensure the num of nodes is the same\n",
    "    mean = train.mean(axis=(0,1,3), keepdims=True)\n",
    "    std = train.std(axis=(0,1,3), keepdims=True)\n",
    "    print('mean.shape:',mean.shape)\n",
    "    print('std.shape:',std.shape)\n",
    "\n",
    "    def normalize(x):\n",
    "        return (x - mean) / std\n",
    "\n",
    "    train_norm = normalize(train)\n",
    "    val_norm = normalize(val)\n",
    "    test_norm = normalize(test)\n",
    "\n",
    "    return {'_mean': mean, '_std': std}, train_norm, val_norm, test_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "869337df-a978-4a1d-aae5-a1f4ac0e2cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean.shape: (1, 1, 3, 1)\n",
      "std.shape: (1, 1, 3, 1)\n"
     ]
    }
   ],
   "source": [
    "train_x = np.concatenate(training_set[:-2], axis=-1)  # (B,N,F,T')\n",
    "val_x = np.concatenate(validation_set[:-2], axis=-1)\n",
    "test_x = np.concatenate(testing_set[:-2], axis=-1)\n",
    "\n",
    "train_target = training_set[-2]  # (B,N,T)\n",
    "val_target = validation_set[-2]\n",
    "test_target = testing_set[-2]\n",
    "\n",
    "train_timestamp = training_set[-1]  # (B,1)\n",
    "val_timestamp = validation_set[-1]\n",
    "test_timestamp = testing_set[-1]\n",
    "\n",
    "(stats, train_x_norm, val_x_norm, test_x_norm) = normalization(train_x, val_x, test_x)\n",
    "\n",
    "all_data = {'train': { 'x': train_x_norm, 'target': train_target,'timestamp': train_timestamp},\n",
    "            'val': {'x': val_x_norm, 'target': val_target, 'timestamp': val_timestamp},\n",
    "            'test': {'x': test_x_norm, 'target': test_target, 'timestamp': test_timestamp},\n",
    "            'stats': {'_mean': stats['_mean'], '_std': stats['_std']} }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ef908a35-c2db-4287-bf5f-99a6dc0fad8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train x: (10185, 307, 3, 12)\n",
      "train target: (10185, 307, 6)\n",
      "train timestamp: (10185, 1)\n",
      "\n",
      "val x: (3395, 307, 3, 12)\n",
      "val target: (3395, 307, 6)\n",
      "val timestamp: (3395, 1)\n",
      "\n",
      "test x: (3395, 307, 3, 12)\n",
      "test target: (3395, 307, 6)\n",
      "test timestamp: (3395, 1)\n",
      "\n",
      "train data _mean : (1, 1, 3, 1) [[[[2.07264182e+02]\n",
      "   [5.13353675e-02]\n",
      "   [6.34715779e+01]]]]\n",
      "train data _std : (1, 1, 3, 1) [[[[1.56484182e+02]\n",
      "   [4.78711936e-02]\n",
      "   [8.10706255e+00]]]]\n"
     ]
    }
   ],
   "source": [
    "print('train x:', all_data['train']['x'].shape)\n",
    "print('train target:', all_data['train']['target'].shape)\n",
    "print('train timestamp:', all_data['train']['timestamp'].shape)\n",
    "print()\n",
    "print('val x:', all_data['val']['x'].shape)\n",
    "print('val target:', all_data['val']['target'].shape)\n",
    "print('val timestamp:', all_data['val']['timestamp'].shape)\n",
    "print()\n",
    "print('test x:', all_data['test']['x'].shape)\n",
    "print('test target:', all_data['test']['target'].shape)\n",
    "print('test timestamp:', all_data['test']['timestamp'].shape)\n",
    "print()\n",
    "print('train data _mean :', all_data['stats']['_mean'].shape, all_data['stats']['_mean'])\n",
    "print('train data _std :', all_data['stats']['_std'].shape, all_data['stats']['_std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e6f71140-6b85-49c7-9b66-c35384d09dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save file: .\\PEMS04_046r2_046d0_046w0_astcgn\n"
     ]
    }
   ],
   "source": [
    "file = os.path.basename(graph_signal_matrix_filename).split('.')[0]\n",
    "dirpath = '.'\n",
    "filename = os.path.join(dirpath, file + '_046r' + str(num_of_half_hours) + '_046d' + str(num_of_days) + '_046w' + str(num_of_weeks)) + '_astcgn'\n",
    "print('save file:', filename)\n",
    "np.savez_compressed(filename,\n",
    "                train_x=all_data['train']['x'],train_target=all_data['train']['target'],train_timestamp=all_data['train']['timestamp'],\n",
    "                val_x=all_data['val']['x'], val_target=all_data['val']['target'],val_timestamp=all_data['val']['timestamp'],\n",
    "                test_x=all_data['test']['x'], test_target=all_data['test']['target'], test_timestamp=all_data['test']['timestamp'],\n",
    "                mean=all_data['stats']['_mean'], std=all_data['stats']['_std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e4d6f7-710b-4af5-83f1-1a93df4c1cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
